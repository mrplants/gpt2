import csv
import torch
from torch.utils.data import Dataset
from torch.nn.utils.rnn import pad_sequence
from transformers import GPT2Tokenizer
import os

class AbstractDataset(Dataset):
    """Defines a Dataset for the Kaggle arXiv abstract dataset.
    
    This class tokenizes and encodes the abstracts in the dataset using the
    GPT-2 tokenizer.
    
    Attributes:
        tokenizer:  The GPT-2 tokenizer.
        data:  A list of encoded abstracts.
    """
    def __init__(self) -> None:
        super().__init__()
        # Use GPT-2 tokenizer
        self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

        self.data = []
        # Read the arxiv data from the CSV file
        # Using relative path, assuming we are running the script from the gpt2
        # directory
        with open('./sample_data/arxiv_data.csv', 'r') as csv_in:
            csv_reader = csv.reader(csv_in)
            next(csv_reader)  # Skip the header
            # Read the rows from the csv file
            for row in csv_reader:
                # Tokenize and encode the abstracts, then append to the data
                # list
                self.data.append(' '.join(row[1].split('\n')))
    
    def __len__(self):
        """Returns the length of the dataset."""
        return len(self.data)
    
    def __getitem__(self, index):
        """Returns the encoded abstract at the given index."""
        return torch.tensor(self.tokenizer.encode(self.data[index]))

def collate_fn(batch):
    """Processes the data generated by the AbstractDataset
    
    Pads the sequences to the same length with 0s.
    
    Args:
        batch:  A list of encoded abstracts.
        
    Returns:
        The padded batch of encoded abstracts."""
    return pad_sequence(batch, batch_first=True, padding_value=0)